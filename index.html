<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Yaodong Yu</title>
</head>
<body style="width: 1000px; margin: 0 auto;">
<div id="fwtitle">
<div id="toptitle">
<h1>Yaodong Yu</h1>
</div>
</div>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="photo_me_2.jpg" alt="photo_me" width="190px" height="200px" />&nbsp;</td>
<td align="left"><p>Graduate Student<br />
EECS Department, University of California, Berkeley</p>
<p>7th floor, Sutardja Dai Hall, Berkeley, CA 94720.<br />  
Email: yyu AT eecs DOT berkeley DOT edu</a>

</p>
</td></tr></table>
<h2>About me</h2>
<p>I am a final-year PhD student in the EECS department at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~jordan/">Michael I. Jordan</a> and <a href="http://people.eecs.berkeley.edu/~yima/">Yi Ma</a>. I obtained my B.S. from the Department of Mathematics at Nanjing University, and my M.S. from the Department of Computer Science</a>, University of Virginia. 
<!-- Previously, I was fortunate to work with <a href="http://www.ntu.edu.sg/home/sinnopan/">Sinno Jialin Pan</a> at Nanyang Technological University.  -->

</p>
My research interests are broadly in theoretical foundations and applications of trustworthy machine learning. My current focus is on 
<!-- My research interests include topics in machine learning and optimization. My goal is to make machine learning systems more robust. -->
</p>
<ul>
<li>Interpretable white-box deep neural networks [e.g., <a href="https://arxiv.org/abs/2306.01129">CRATE: white-box transformer</a>];
</li>
<li>Differentially private foundation models [e.g., <a href="https://arxiv.org/abs/2306.08842">ViP: A Differentially Private Vision Foundation Model</a>];
</li>
<li>Optimization and uncertainty quantification for collaborative (federated) learning [e.g., <a href="https://proceedings.mlr.press/v202/lu23i">FCP: Federated Conformal Predictors</a>];
</li>
<li>Robustness under distribution shifts [e.g., <a href="https://proceedings.mlr.press/v162/yu22i.html">ProjNorm: Predicting Out-of-Distribution Error</a>].
</li>
</ul>

<!-- <b>I'm on the 2023-2024 academic job market.</b> -->



<!-- <h2>News</h2>

<ul>
	   <li>
	   	[11/9/2018] We won the <b>1st place</b> in both Robust Model Track and Targeted Attack Track, as well as <b>the 3rd place</b> in Untargeted Attack Track in <a href="https://www.crowdai.org/challenges/nips-2018-adversarial-vision-challenge-robust-model-track">NeurIPS 2018 : Adversarial Vision Challenge</a> [<a href="https://medium.com/bethgelab/results-of-the-nips-adversarial-vision-challenge-2018-e1e21b690149">News Link</a>]. Congratulations to our team! Please check our new method <b>TRADES</b> [<a href="https://arxiv.org/abs/1901.08573">paper</a>] [<a href="https://github.com/yaodongyu/TRADES">code</a>].
       </li>
</ul> -->

<h2>Recent Publications [<a href="https://scholar.google.com/citations?user=bZ9oyW8AAAAJ&hl=zh-CN&oi=ao">Google Scholar</a>] </h2> 
       (*: equal contribution)
<!-- <h3>Preprints</h3>
<ul>
<li><p><i>Saving Gradient and Negative Curvature Computations: Finding Local Minima More Efficiently</i>.<br /> 
<b>Yaodong Yu</b>*, Difan Zou* and Quanquan Gu (*: equal contribution). arXiv:1712.03950, 2017. [<a href="https://arxiv.org/abs/1712.03950">arXiv</a>]<br /></p>
</li>
</ul> -->

<!-- <h3>Conference Proceedings</h3> -->
<ul>
<li><p><i>White-Box Transformers via Sparse Rate Reduction</i>.<br />
<b>Yaodong Yu</b>, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin D. Haeffele, Yi Ma. <br />  
In Proceedings of the 37th Conference on Advances in Neural Information Processing Systems (NeurIPS'2023), 2023. [<a href="https://arxiv.org/abs/2306.01129">arxiv</a>] [<a href="https://github.com/Ma-Lab-Berkeley/CRATE">code</a>]<br /></p>
</li>
</ul>
<ul>
<li><p><i>ViP: A Differentially Private Foundation Model for Computer Vision</i>.<br />
<b>Yaodong Yu</b>, Maziar Sanjabi, Yi Ma, Kamalika Chaudhuri, Chuan Guo. <br />  
Preliminary version presented at Theory and Practice of Differential Privacy Workshop (TPDP'2023), 2023. [<a href="https://arxiv.org/abs/2306.08842">arXiv</a>] [<a href="https://github.com/facebookresearch/ViP-MAE">code</a>]<br /></p>
</li>
</ul>
<ul>
<li><p><i>Federated Conformal Predictors for Distributed Uncertainty Quantification</i>.<br />
Charles Lu*, <b>Yaodong Yu*</b>, Sai Praneeth Karimireddy, Michael Jordan, Ramesh Raskar. <br />  
In Proceedings of the 40th International Conference on Machine Learning (ICML'2023), 2023. [<a href="https://proceedings.mlr.press/v202/lu23i.html">link</a>] [<a href="https://github.com/clu5/federated-conformal">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Robust Calibration with Multi-domain Temperature Scaling</i>.<br />
<b>Yaodong Yu</b>, Stephen Bates, Yi Ma, Michael I. Jordan. <br />  
In Proceedings of the 36th Conference on Advances in Neural Information Processing Systems (NeurIPS'2022), 2022. [<a href="https://openreview.net/pdf?id=UZJHudsQ7d">link</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>TCT: Convexifying Federated Learning using Bootstrapped Neural Tangent Kernels</i>.<br />
<b>Yaodong Yu</b>, Alexander Wei, Sai Praneeth Karimireddy, Yi Ma, Michael I. Jordan. <br />  
In Proceedings of the 36th Conference on Advances in Neural Information Processing Systems (NeurIPS'2022), 2022. [<a href="https://openreview.net/pdf?id=jzd2bE5MxW">link</a>] [<a href="https://github.com/yaodongyu/TCT">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Predicting Out-of-Distribution Error with the Projection Norm</i>.<br />
<b>Yaodong Yu</b>*, Zitong Yang*, Alexander Wei, Yi Ma, Jacob Steinhardt. <br />  
In Proc. of the 39th International Conference on Machine Learning (ICML'2022), 2022. [<a href="https://proceedings.mlr.press/v162/yu22i.html">link</a>] [<a href="https://github.com/yaodongyu/ProjNorm">code</a>] <br /></p>
</li>
</ul>
<!-- <ul>
<li><p><i>What You See is What You Get: Distributional Generalization for Algorithm Design in Deep Learning</i>.<br />
Bogdan Kulynych*, Yao-Yuan Yang*, <b>Yaodong Yu</b>, Jarosław Błasiok, Preetum Nakkiran. <br />  
In Proc. of the 36th Conference on Advances in Neural Information Processing Systems (NeurIPS'2022), 2022. [<a href="https://arxiv.org/abs/2204.03230">arXiv</a>] [<a href="https://github.com/yangarbiter/dp-dg">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Conditional Supervised Contrastive Learning for Fair Text Classification</i>.<br />
Jianfeng Chi, William Shand, <b>Yaodong Yu</b>, Kai-Wei Chang, Han Zhao and Yuan Tian. <br />  
In Proceedings of the 2022 Empirical Methods in Natural Language Processing (EMNLP'2022 Findings), 2022. [<a href="https://arxiv.org/abs/2205.11485">arXiv</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Predicting Out-of-Distribution Error with the Projection Norm</i>.<br />
<b>Yaodong Yu</b>*, Zitong Yang*, Alexander Wei, Yi Ma, Jacob Steinhardt. <br />  
In Proc. of the 39th International Conference on Machine Learning (ICML'2022), 2022. [<a href="https://proceedings.mlr.press/v162/yu22i.html">Link</a>] [<a href="https://proceedings.mlr.press/v162/yu22i/yu22i.pdf">PDF</a>] [<a href="https://github.com/yaodongyu/ProjNorm">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Online Nonsubmodular Minimization with Delayed Costs: From Full Information to Bandit Feedback</i>.<br />
Tianyi Lin*, Aldo Pacchiano*, <b>Yaodong Yu</b>*, Michael I. Jordan. <br />  
In Proc. of the 39th International Conference on Machine Learning (ICML'2022), 2022. [<a href="https://proceedings.mlr.press/v162/lin22g.html">Link</a>] [<a href="https://proceedings.mlr.press/v162/lin22g/lin22g.pdf">PDF</a>]  <br /></p>
</li>
</ul>
<ul>
<li><p><i>ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction</i>.<br />
Kwan Ho Ryan Chan*, <b>Yaodong Yu</b>*, Chong You*, Haozhi Qi, John Wright, Yi Ma. <br />  
Journal of Machine Learning Research (JMLR'2022), 2022. [<a href="https://arxiv.org/abs/2105.10446">arXiv</a>] [<a href="https://www.jmlr.org/papers/v23/21-0631.html">Link</a>] [<a href="https://arxiv.org/pdf/2105.10446.pdf">PDF</a>]  [<a href="https://github.com/Ma-Lab-Berkeley/ReduNet">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Fast Distributionally Robust Learning with Variance-Reduced Min-Max Optimization</i>.<br />
<b>Yaodong Yu</b>*, Tianyi Lin*, Eric Mazumdar*, Michael I. Jordan. <br />  
In Proc. of the 25nd of the International Conference on Artificial Intelligence and Statistics (AISTATS'2022), 2022. [<a href="https://proceedings.mlr.press/v151/yu22a">Link</a>] [<a href="https://proceedings.mlr.press/v151/yu22a/yu22a.pdf">PDF</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging</i>.<br />
Chris Junchi Li*, <b>Yaodong Yu</b>*, Nicolas Loizou, Gauthier Gidel, Yi Ma, Nicolas Le Roux, Michael I. Jordan. <br />  
In Proc. of the 25nd of the International Conference on Artificial Intelligence and Statistics (AISTATS'2022), 2022. [<a href="https://proceedings.mlr.press/v151/junchi-li22a.html">Link</a>] [<a href="https://proceedings.mlr.press/v151/junchi-li22a/junchi-li22a.pdf">PDF</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Adversarial Robustness of Stabilized NeuralODEs Might be from Obfuscated Gradients</i>.<br />  Yifei Huang, <b>Yaodong Yu</b>, Hongyang Zhang, Yi Ma, and Yuan Yao.<br />  Mathematical and Scientific Machine Learning (MSML'2021), 2021. [<a href="https://proceedings.mlr.press/v145/huang22a.html">Link</a>] [<a href="https://proceedings.mlr.press/v145/huang22a/huang22a.pdf">PDF</a>] [<a href="https://github.com/yao-lab/SONet">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction</i>.<br />  <b>Yaodong Yu</b>*, Kwan Ho Ryan Chan*, Chong You, Chaobing Song, Yi Ma.<br />  In Proc. of the 34th Conference on Advances in Neural Information Processing Systems (NeurIPS'2020), 2020. [<a href="https://arxiv.org/abs/2006.08558">arXiv</a>] [<a href="https://proceedings.neurips.cc/paper/2020/hash/6ad4174eba19ecb5fed17411a34ff5e6-Abstract.html">Link</a>] [<a href="https://proceedings.neurips.cc/paper/2020/file/6ad4174eba19ecb5fed17411a34ff5e6-Paper.pdf">PDF</a>] [<a href="https://github.com/ryanchankh/mcr2">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Boundary Thickness and Robustness in Learning Models</i>.<br /> Yaoqing Yang, Rajiv Khanna, <b>Yaodong Yu</b>, Amir Gholami, Kurt Keutzer, Joseph Gonzalez, Kannan Ramchandran, and Michael W Mahoney.<br />  In Proc. of the 34th Conference on Advances in Neural Information Processing Systems (NeurIPS'2020), 2020. [<a href="https://arxiv.org/abs/2007.05086">arXiv</a>] [<a href="https://papers.nips.cc/paper/2020/hash/44e76e99b5e194377e955b13fb12f630-Abstract.html">Link</a>] [<a href="https://proceedings.neurips.cc/paper/2020/file/44e76e99b5e194377e955b13fb12f630-Paper.pdf">PDF</a>] [<a href="https://github.com/nsfzyzz/boundary_thickness">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Rethinking Bias-Variance Trade-off for Generalization of Neural Networks</i>.<br /> Zitong Yang*, <b>Yaodong Yu</b>*, Chong You, Jacob Steinhardt, and Yi Ma.<br />  In Proc. of the 37th International Conference on Machine Learning (ICML'2020), 2020. [<a href="https://arxiv.org/abs/2002.11328">arXiv</a>] [<a href="https://proceedings.mlr.press/v119/yang20j">Link</a>] [<a href="http://proceedings.mlr.press/v119/yang20j/yang20j.pdf">PDF</a>] [<a href="https://github.com/yaodongyu/Rethink-BiasVariance-Tradeoff">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Theoretically Principled Trade-off between Robustness and Accuracy</i>.<br /> Hongyang Zhang, <b>Yaodong Yu</b>, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui and Michael I. Jordan. In Proc. of the 36th International Conference on Machine Learning (ICML'2019), Long Beach, California, USA, 2019. [<a href="https://arxiv.org/abs/1901.08573">arXiv</a>] [<a href="http://proceedings.mlr.press/v97/zhang19p">Link</a>] [<a href="http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf">PDF</a>] [<a href="https://github.com/yaodongyu/TRADES">code</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>Learning One-hidden-layer ReLU Networks via Gradient Descent</i>.<br /> 
Xiao Zhang*, <b>Yaodong Yu</b>*, Lingxiao Wang* and Quanquan Gu. <br /> 
In Proc. of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS'2019), Naha, Okinawa, Japan, 2019. [<a href="http://proceedings.mlr.press/v89/zhang19g">Link</a>] [<a href="http://proceedings.mlr.press/v89/zhang19g/zhang19g.pdf">PDF</a>]
</li>
</ul>
<ul>
<li><p><i>Third-order Smoothness Helps: Faster Stochastic Optimization Algorithms for Finding Local Minima</i>.<br /> 
<b>Yaodong Yu</b>*, Pan Xu* and Quanquan Gu.<br />
In Proc. of the 32nd Conference on Advances in Neural Information Processing Systems (NeurIPS'2018), Montréal, Canada, 2018. [<a href="http://papers.nips.cc/paper/7704-third-order-smoothness-helps-faster-stochastic-optimization-algorithms-for-finding-local-minima">Link</a>] [<a href="http://papers.nips.cc/paper/7704-third-order-smoothness-helps-faster-stochastic-optimization-algorithms-for-finding-local-minima.pdf">PDF</a>][<a href="files_pdf/poster_FLASH.pdf">Poster</a>] <br /></p>
</li>
</ul>
<ul>
<li><p><i>A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank Matrix Recovery</i>.<br /> 
Xiao Zhang*, Lingxiao Wang*, <b>Yaodong Yu</b> and Quanquan Gu.<br />
In Proc. of the 35th International Conference on Machine Learning (ICML'2018), Stockholm, Sweden, 2018. [<a href="http://proceedings.mlr.press/v80/zhang18m.html">Link</a>] [<a href="http://proceedings.mlr.press/v80/zhang18m/zhang18m.pdf">PDF</a>]<br /></p>
</li>
</ul>
<ul>
<li><p><i>Data Poisoning Attacks on Multi-Task Relationship Learning</i>.<br /> 
Mengchen Zhao, Bo An, <b>Yaodong Yu</b>, Sulin Liu and Sinno Jialin Pan.<br />
In Proc. of the 32nd AAAI Conference on Artificial Intelligence (AAAI'2018), New Orleans, Louisiana, USA, 2018. [<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16073">Link</a>] [<a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16073/16778">PDF</a>]<br /></p>
</li>
</ul>
<ul>
<li><p><i>Communication-Efficient Distributed Primal-Dual Algorithm for Saddle Point Problems</i>.<br /> 
<b>Yaodong Yu</b>*, Sulin Liu* and Sinno Jialin Pan.<br />
In Proc. of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI'2017), Sydney, Australia, 2017. [<a href="http://auai.org/uai2017/proceedings/papers/286.pdf">Link</a>] [<a href="http://auai.org/uai2017/proceedings/papers/286.pdf">PDF</a>]<br /></p>
</li>
</ul> -->


</p>
</li>
</ul>
<h2>Professional Experiences</h2>
<ul>
<li><p><b>Visiting Researcher</b>, <a href="https://ai.facebook.com">FAIR at Meta</a>, San Francisco, Oct. 2022 &ndash; Sep. 2024<br />
</li>
<li><p><b>Research Intern</b>, <a href="https://ai.facebook.com">Meta AI</a>, San Francisco, May. 2022 &ndash; Oct. 2022<br />
</li>
<li><p><b>Research Intern</b>, <a href="https://research.google">Google Research</a>, Remote, May. 2021 &ndash; Aug. 2021<br />
</li>
<li><p><b>Research Intern</b>, <a href="http://petuum.com">Petuum</a>, Pittsburgh, PA, May. 2018 &ndash; Dec. 2018<br />
</li>
<li><p><b>Research Assistant</b>, <a href="http://scse.ntu.edu.sg/Pages/Home.aspx">School of Computer Science and Engineering</a>, <a href="http://www.ntu.edu.sg/Pages/home.aspx">Nanyang Technological University (NTU)</a>, Singapore, Sep. 2016 &ndash; Aug. 2017<br />
</li>
<!-- <li><p><b>Machine Learning Engineer</b>, <a href="https://www.wecash.net">Wecash</a>, Beijing, China, June 2015 &ndash; May. 2016<br />
</li> -->
</ul>




</div>
</body>
</html>
